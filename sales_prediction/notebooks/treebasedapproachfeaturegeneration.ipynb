{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective:\n",
    "Here we compute all features except text based features. Text based features are computed separately in a different notebook. Features computed here includes\n",
    "1. Mean encodings on item_id, shop_id, category_id etc.\n",
    "2. Lag features.\n",
    "3. Rolling features.\n",
    "4. Shop's city based features (latitude, longitude, city importance)\n",
    "5. Oldness features based on item_id, shop_id: For how long an item has been trading in the market, or for how long an item has been trading in a specific shop.\n",
    "6. Price based features.\n",
    "\n",
    "Here, control flow is that we compute all features inside of **ModelData class** except mean encodings. Mean encodings are computed at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['addingzerototrain', 'competitive-data-science-predict-future-sales']\n",
      "['__notebook__.ipynb', '__output__.json', 'train_with_zero.hdf', 'custom.css', '__results__.html']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "print(os.listdir('../input/addingzerototrain/'))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FNAME='DATA.hdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.read_hdf('../input/addingzerototrain/train_with_zero.hdf','df')\n",
    "# sales = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\n",
    "items = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\n",
    "shops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\n",
    "categ = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\n",
    "test = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv')\n",
    "ITEM_EN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.loc[sales['item_price'] < 0, 'item_price'] = 0\n",
    "sales.loc[sales['item_cnt_day'] > 300, 'item_cnt_day'] = 300\n",
    "sales.loc[sales['item_cnt_day'] < 0, 'item_cnt_day'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# English translation of categories.\n",
    "CATEGORIES_EN = [\n",
    "    'PC - Headsets / Headphones',\n",
    "    'Accessories - PS2',\n",
    "    'Accessories - PS3',\n",
    "    'Accessories - PS4',\n",
    "    'Accessories - PSP',\n",
    "    'Accessories - PSVita',\n",
    "    'Accessories - XBOX 360',\n",
    "    'Accessories - XBOX ONE',\n",
    "    'Tickets (Digital)',\n",
    "    'Delivery of goods',\n",
    "    'Game Consoles - PS2',\n",
    "    'Game Consoles - PS3',\n",
    "    'Game Consoles - PS4',\n",
    "    'Gaming Consoles - PSP',\n",
    "    'Game Consoles - PSVita',\n",
    "    'Gaming Consoles - XBOX 360',\n",
    "    'Gaming Consoles - XBOX ONE',\n",
    "    'Game Consoles - Other',\n",
    "    'Games - PS2',\n",
    "    'Games - PS3',\n",
    "    'Games - PS4',\n",
    "    'Games - PSP',\n",
    "    'Games - PSVita',\n",
    "    'Games - XBOX 360',\n",
    "    'Games - XBOX ONE',\n",
    "    'Games - Accessories for games',\n",
    "    'Android Games - Digital',\n",
    "    'MAC Games - Digit',\n",
    "    'PC Games - Additional Editions',\n",
    "    'PC Games - Collectible Editions',\n",
    "    'PC Games - Standard Editions',\n",
    "    'PC Games - Digital',\n",
    "    'Payment cards (Cinema, Music, Games)',\n",
    "    'Payment Cards - Live!',\n",
    "    'Payment Cards - Live! (Numeral)',\n",
    "    'Payment Cards - PSN',\n",
    "    'Payment Cards - Windows (Digital)',\n",
    "    'Cinema - Blu-Ray',\n",
    "    'The Movie - Blu-Ray 3D',\n",
    "    'Cinema - Blu-Ray 4K',\n",
    "    'Cinema - DVD',\n",
    "    'Cinema - Collection',\n",
    "    'Books - Artbooks, encyclopedias',\n",
    "    'Books - Audiobooks',\n",
    "    'Books - Audiobooks (Figure)',\n",
    "    'Books - Audiobooks 1C',\n",
    "    'Books - Business Literature',\n",
    "    'Books - Comics, Manga',\n",
    "    'Books - Computer Literature',\n",
    "    'Books - Methodical materials 1C',\n",
    "    'Books - Postcards',\n",
    "    'Books - Cognitive Literature',\n",
    "    'Книги - Путеводители',\n",
    "    'Books - Fiction',\n",
    "    'Books - The Figure',\n",
    "    'Music - Local Production CD',\n",
    "    'Music - CD branded production',\n",
    "    'Music - MP3',\n",
    "    'Music - Vinyl',\n",
    "    'Music - Music Video',\n",
    "    'Music - Gift Edition',\n",
    "    'Gifts - Attributes',\n",
    "    'Gifts - Gadgets, Robots, Sports',\n",
    "    'Gifts - Soft Toys',\n",
    "    'Gifts - Board Games',\n",
    "    'Gifts - Board Games (Compact)',\n",
    "    'Gifts - Cards, stickers',\n",
    "    'Gifts - Development',\n",
    "    'Gifts - Certificates, Services',\n",
    "    'Gifts - Souvenirs',\n",
    "    'Gifts - Souvenirs (in a hitch)',\n",
    "    'Gifts - Bags, Albums, Mouse pads',\n",
    "    'Gifts - Figures',\n",
    "    'Programs - 1C: Enterprise 8',\n",
    "    'Programs - MAC (Digit)',\n",
    "    'Programs - Home and Office',\n",
    "    'Programs - Home and Office (Digital)',\n",
    "    'Programs - Educational',\n",
    "    'Programs - Educational (Figure)',\n",
    "    'Service',\n",
    "    'Service Tickets',\n",
    "    'Clean media (spire)',\n",
    "    'Clean Media (Piece)',\n",
    "    'Batteries',\n",
    "]\n",
    "# English translation of shops names.\n",
    "SHOPS_EN = [\n",
    "    '! Yakutsk Ordzhonikidze, 56 fran',\n",
    "    ' ! Yakutsk Shopping Center \"Central\" fran',\n",
    "    ' Adygea shopping center \"Mega\"',\n",
    "    ' Balashikha October \"Kinomir\"',\n",
    "    ' Volzhsky shopping center \"Volga Mall\"',\n",
    "    ' Vologda shopping center \"Marmalade\"',\n",
    "    ' Voronezh (Plekhanovskaya, 13)',\n",
    "    ' Voronezh TRC \"Maksimir\"',\n",
    "    ' Voronezh TRC City-Park \"Grad\"',\n",
    "    ' Outbound Trading',\n",
    "    ' Zhukovsky st. Chkalov 39m?',\n",
    "    ' Zhukovsky st. Chkalov 39m²',\n",
    "    ' Online Emergency Store',\n",
    "    ' Kazan TC \"Bahetle\"',\n",
    "    ' Kazan TC \"ParkHouse\" II',\n",
    "    ' Kaluga TRC \"XXI century\"',\n",
    "    ' Kolomna TC \"Rio\"',\n",
    "    ' Krasnoyarsk shopping center \"Vzletka Plaza\"',\n",
    "    ' Krasnoyarsk shopping center \"June\"',\n",
    "    ' Kursk TC \"Pushkinsky\"',\n",
    "    ' Moscow \"Sale\"',\n",
    "    ' Moscow MTRC \"Afi Mall\"',\n",
    "    ' Moscow Shop S21',\n",
    "    ' Moscow TC \"Budenovsky\" (pav.A2)',\n",
    "    ' Moscow TC \"Budenovsky\" (pav. K7)',\n",
    "    ' Moscow TRK \"Atrium\"',\n",
    "    ' Moscow TC \"Areal\" (Belyaevo)',\n",
    "    ' Moscow TC \"MEGA Belaya Dacha II\"',\n",
    "    ' Moscow TC \"MEGA Teply Stan\" II',\n",
    "    ' Moscow shopping center \"New Century\" (Novokosino)',\n",
    "    ' Moscow TPP \"Perlovski\"',\n",
    "    ' Moscow shopping center \"Semenovsky\"',\n",
    "    ' Moscow shopping center \"Silver House\"',\n",
    "    ' Mytishchi TRC \"XL-3\"',\n",
    "    ' N.Novgorod SEC \"RIO\"',\n",
    "    ' N.Novgorod SEC \"Fantastic\"',\n",
    "    ' Novosibirsk SEC \"Gallery Novosibirsk\"',\n",
    "    ' Novosibirsk TC \"Mega\"',\n",
    "    ' Omsk TC \"Mega\"',\n",
    "    ' RostovNaDon TRK \"Megacenter Horizon\"',\n",
    "    ' RostovNaDonu TRK \"Megacenter Horizon\" Ostrovnoy',\n",
    "    ' RostovNaDonu TC \"Mega\"',\n",
    "    ' St. Petersburg TC \"Nevsky Center\"',\n",
    "    ' St. Petersburg TC \"Sennaya\"',\n",
    "    ' Samara TP \"Melody\"',\n",
    "    ' Samara TC \"ParkHaus\"',\n",
    "    ' Sergiev Posad TC 7YA',\n",
    "    ' Surgut SEC \"City Mall\"',\n",
    "    ' Tomsk SEC \"Emerald City\"',\n",
    "    ' Tyumen SEC \"Crystal\"',\n",
    "    ' Tyumen shopping center \"Goodwin\"',\n",
    "    ' Tyumen shopping center \"Green Coast\"',\n",
    "    ' Ufa TC \"Central\"',\n",
    "    ' Ufa shopping center \"Family\" 2',\n",
    "    ' Khimki TC \"Mega\"',\n",
    "    ' Digital warehouse 1C-Online',\n",
    "    ' Chekhov SEC \"Carnival\"',\n",
    "    ' Yakutsk Ordzhonikidze, 56',\n",
    "    ' Yakutsk shopping center \"Central\"',\n",
    "    ' Yaroslavl shopping center \"Altair',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City data for shops: area, city_id, importance, latitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy import distance\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "CITY_DATA = {\n",
    "    'Калуга': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.657295410002972,\n",
    "        'area': 358.911060027607,\n",
    "        'lat': 54.5101087,\n",
    "        'lon': 36.2598115\n",
    "    },\n",
    "    'Волжский': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.550220481385513,\n",
    "        'area': 438.60566745306613,\n",
    "        'lat': 48.782102,\n",
    "        'lon': 44.7779843\n",
    "    },\n",
    "    'Воронеж': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.7045722199183749,\n",
    "        'area': 2087.0423692794884,\n",
    "        'lat': 51.6605982,\n",
    "        'lon': 39.2005858\n",
    "    },\n",
    "    'Жуковский': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.5444576798578901,\n",
    "        'area': 84.74916146073933,\n",
    "        'lat': 55.5972801,\n",
    "        'lon': 38.1199863\n",
    "    },\n",
    "    'Самара': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.696449565028584,\n",
    "        'area': 2205.1176282331953,\n",
    "        'lat': 53.198627,\n",
    "        'lon': 50.113987\n",
    "    },\n",
    "    'Коломна': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.648802312001607,\n",
    "        'area': 149.8134437525239,\n",
    "        'lat': 55.0938743,\n",
    "        'lon': 38.7670121\n",
    "    },\n",
    "    'Якутск': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.654131338136001,\n",
    "        'area': 304.64719883743965,\n",
    "        'lat': 62.027287,\n",
    "        'lon': 129.732086\n",
    "    },\n",
    "    'Ярославль': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.7065300207968069,\n",
    "        'area': 455.605256943589,\n",
    "        'lat': 57.6263877,\n",
    "        'lon': 39.8933705\n",
    "    },\n",
    "    'Балашиха': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.604005896721309,\n",
    "        'area': 341.3399072537641,\n",
    "        'lat': 55.8036225,\n",
    "        'lon': 37.9646488\n",
    "    },\n",
    "    '!Якутск': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.654131338136001,\n",
    "        'area': 304.64719883743965,\n",
    "        'lat': 62.027287,\n",
    "        'lon': 129.732086\n",
    "    },\n",
    "    'Новосибирск': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.728185699386452,\n",
    "        'area': 977.968425123674,\n",
    "        'lat': 55.0282171,\n",
    "        'lon': 82.9234509\n",
    "    },\n",
    "    'Тюмень': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.672385913300304,\n",
    "        'area': 1029.734170100062,\n",
    "        'lat': 57.153534,\n",
    "        'lon': 65.542274\n",
    "    },\n",
    "    'Москва': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.922316290384526,\n",
    "        'area': 2220.463036723328,\n",
    "        'lat': 55.7504461,\n",
    "        'lon': 37.6174943\n",
    "    },\n",
    "    'Томск': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.680307551446071,\n",
    "        'area': 378.01721837627406,\n",
    "        'lat': 56.488712,\n",
    "        'lon': 84.952324\n",
    "    },\n",
    "    'Казань': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.741972620362528,\n",
    "        'area': 1317.0415739017471,\n",
    "        'lat': 55.7823547,\n",
    "        'lon': 49.1242266\n",
    "    },\n",
    "    'Курск': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.676179816540548,\n",
    "        'area': 399.62069154089585,\n",
    "        'lat': 51.739433,\n",
    "        'lon': 36.179604\n",
    "    },\n",
    "    'Уфа': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.695077392339603,\n",
    "        'area': 1721.7276870129429,\n",
    "        'lat': 54.726288,\n",
    "        'lon': 55.947727\n",
    "    },\n",
    "    'Вологда': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.661524874526942,\n",
    "        'area': 214.3430466663635,\n",
    "        'lat': 59.218876,\n",
    "        'lon': 39.893276\n",
    "    },\n",
    "    'Ростов На Дону': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.7893194127943861,\n",
    "        'area': 811.0162296285571,\n",
    "        'lat': 47.2213858,\n",
    "        'lon': 39.7114196\n",
    "    },\n",
    "    'Адыгея': {\n",
    "        'type': 'administrative',\n",
    "        'importance': 0.692337665336592,\n",
    "        'area': 27295.264468374502,\n",
    "        'lat': 44.6939006,\n",
    "        'lon': 40.1520421\n",
    "    },\n",
    "    'Химки': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.614359398958108,\n",
    "        'area': 230.77301628863356,\n",
    "        'lat': 55.8892847,\n",
    "        'lon': 37.4449896\n",
    "    },\n",
    "    'Чехов': {\n",
    "        'type': 'town',\n",
    "        'importance': 0.49824054756674396,\n",
    "        'area': 38.796925301165096,\n",
    "        'lat': 55.1426603,\n",
    "        'lon': 37.4545328\n",
    "    },\n",
    "    'Мытищи': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.617191804459829,\n",
    "        'area': 95.08539485314338,\n",
    "        'lat': 55.9094928,\n",
    "        'lon': 37.7339358\n",
    "    },\n",
    "    'Красноярск': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.698696821640559,\n",
    "        'area': 836.7336288346606,\n",
    "        'lat': 56.0090968,\n",
    "        'lon': 92.8725147\n",
    "    },\n",
    "    'Сергиев': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.529283352524288,\n",
    "        'area': 88.864113189393,\n",
    "        'lat': 56.3153529,\n",
    "        'lon': 38.1358208\n",
    "    },\n",
    "    'Сургут': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.617507029143067,\n",
    "        'area': 315.54371943139705,\n",
    "        'lat': 61.254032,\n",
    "        'lon': 73.3964\n",
    "    },\n",
    "    'СПб': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.837718425910333,\n",
    "        'area': 1137.355369029224,\n",
    "        'lat': 59.938732,\n",
    "        'lon': 30.316229\n",
    "    },\n",
    "    'Омск': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.706863149131586,\n",
    "        'area': 2424.721167579471,\n",
    "        'lat': 54.991375,\n",
    "        'lon': 73.371529\n",
    "    },\n",
    "    'Н.Новгород': {\n",
    "        'type': 'signal',\n",
    "        'importance': 0.11100000000000002,\n",
    "        'area': 6.734885040711671e-05,\n",
    "        'lat': 57.1810745,\n",
    "        'lon': 45.1382139\n",
    "    },\n",
    "    'Ни́жний Но́вгород': {\n",
    "        'type': 'city',\n",
    "        'importance': 0.972093333761689,\n",
    "        'area': 570.3511705993303,\n",
    "        'lat': 56.328571,\n",
    "        'lon': 44.003506\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def area_city(geocode):\n",
    "    lat = list(map(float, geocode.raw['boundingbox'][:2]))\n",
    "    lon = list(map(float, geocode.raw['boundingbox'][2:]))\n",
    "    length = distance.distance((lat[0], lon[0]), (lat[0], lon[1])).km\n",
    "    breadth = distance.distance((lat[0], lon[0]), (lat[1], lon[0])).km\n",
    "    return length * breadth\n",
    "\n",
    "def add_city_name(shops_df):\n",
    "    shops_df['city'] = shops_df.shop_name.apply(lambda x: x.split(' ')[0])\n",
    "    shops_df.loc[shops_df.city =='РостовНаДону', 'city'] = 'Ростов На Дону'\n",
    "    shops_df.loc[shops_df.city =='Н.Новгород', 'city'] = 'Ни́жний Но́вгород'\n",
    "\n",
    "\n",
    "class CityData:\n",
    "    def __init__(self, shops_df):\n",
    "        self._shops_df = shops_df.copy()\n",
    "        self.data = {}\n",
    "        self._geolocator = None\n",
    "\n",
    "        add_city_name(self._shops_df)\n",
    "        self._invalid_cities = ['Интернет-магазин', 'Выездная', 'Цифровой']\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        if self._geolocator is None:\n",
    "            self._geolocator = Nominatim(\n",
    "                user_agent=\"sales prediction_{}\".format(np.random.rand()))\n",
    "        cities_ru = set(self._shops_df.city.values)\n",
    "        for city in cities_ru:\n",
    "            if city in self._invalid_cities or city in self.data:\n",
    "                continue\n",
    "\n",
    "            location = self._geolocator.geocode(city + ', Россия')\n",
    "            assert location.raw['type'] in ['city', 'town', 'administrative']\n",
    "            self.data[city] = {\n",
    "                'type': location.raw['type'],\n",
    "                'importance': location.raw['importance'],\n",
    "                'area': area_city(location),\n",
    "                'lat': location.latitude,\n",
    "                'lon': location.longitude\n",
    "            }\n",
    "\n",
    "            print(city, location.raw['type'],\n",
    "                  round(location.raw['importance'], 2),\n",
    "                  (location.latitude, location.longitude))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from city_data import CITY_DATA, add_city_name\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def add_city_data_features(X_df, shops_df):\n",
    "    \"\"\"\n",
    "    Latitude, Longitude, Area and Importance of cities is added\n",
    "    \"\"\"\n",
    "    shops_df = shops_df.copy()\n",
    "    add_city_name(shops_df)\n",
    "    shops_df['city_id'] = LabelEncoder().fit_transform(shops_df['city']).astype(np.int16)\n",
    "    city_features = ['city_id']\n",
    "    for key in ['lat', 'lon', 'importance', 'area']:\n",
    "        feature = 'city_' + key\n",
    "        shops_df[feature] = shops_df['city'].apply(lambda x: CITY_DATA[x][key] if x in CITY_DATA else -1).astype(\n",
    "            np.float32)\n",
    "        city_features.append(feature)\n",
    "\n",
    "    X_df = X_df.reset_index()\n",
    "    X_df = pd.merge(X_df, shops_df[city_features + ['shop_id']], how='left', on='shop_id')\n",
    "    X_df.set_index('index', inplace=True)\n",
    "    return X_df\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     from constants import SALES_FPATH, SHOPS_FPATH\n",
    "#     sales_df = pd.read_csv(SALES_FPATH)\n",
    "#     shops_df = pd.read_csv(SHOPS_FPATH)\n",
    "\n",
    "#     print(add_city_data_features(sales_df, shops_df).head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Id features processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class IdFeatures:\n",
    "    ABSENT_ITEM_ID_VALUE = -1000\n",
    "\n",
    "    def __init__(self, sales_df: pd.DataFrame, items_df: pd.DataFrame):\n",
    "\n",
    "        self._sales_df = sales_df\n",
    "        self._items_df = items_df\n",
    "\n",
    "        # first time occuring features\n",
    "        self._item_fm_df = None\n",
    "        self._item_shop_fm_df = None\n",
    "\n",
    "    def _fit_first_time_occuring_features(self):\n",
    "        assert 'orig_item_id' in self._sales_df\n",
    "\n",
    "        temp_df = self._sales_df[self._sales_df.item_cnt_day > 0][['orig_item_id', 'shop_id', 'date_block_num']]\n",
    "\n",
    "        self._item_fm_df = temp_df.groupby(['orig_item_id'])['date_block_num'].min().to_frame('fm').reset_index()\n",
    "        self._item_shop_fm_df = temp_df.groupby(['orig_item_id',\n",
    "                                                 'shop_id'])['date_block_num'].min().to_frame('fm').reset_index()\n",
    "        assert 'orig_item_id' in self._item_fm_df\n",
    "        assert 'orig_item_id' in self._item_shop_fm_df\n",
    "\n",
    "    def get_fm_features(self, df, item_id_and_shop_id=False):\n",
    "        \"\"\"\n",
    "        Adds  first month features to df.\n",
    "        df must have ['orig_item_id','shop_id','date_block_num'] columns\n",
    "        \"\"\"\n",
    "        assert 'orig_item_id' in df\n",
    "        if self._item_fm_df is None:\n",
    "            self._fit_first_time_occuring_features()\n",
    "\n",
    "        merge_df = self._item_shop_fm_df if item_id_and_shop_id else self._item_fm_df\n",
    "        on_columns = ['orig_item_id', 'shop_id'] if item_id_and_shop_id else ['orig_item_id']\n",
    "        f_nm_prefix = '_'.join(on_columns) + '_'\n",
    "        df = pd.merge(df.reset_index(), merge_df, on=on_columns, how='left').set_index('index')\n",
    "\n",
    "        old_col = f_nm_prefix + 'oldness'\n",
    "        fm_col = f_nm_prefix + 'is_fm'\n",
    "\n",
    "        df[old_col] = df['date_block_num'] - df['fm']\n",
    "        # We will set oldness to 0 for which we don't have the data.\n",
    "        df[old_col] = df[old_col].fillna(0).astype(int)\n",
    "\n",
    "        df[fm_col] = df[old_col] == 0\n",
    "\n",
    "        return df.drop('fm', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "from typing import Union\n",
    "\n",
    "\n",
    "def get_datetime(date_block_num):\n",
    "    year = 2013 + date_block_num // 12\n",
    "    month = date_block_num % 12 + 1\n",
    "    return datetime(year, month, 1)\n",
    "\n",
    "\n",
    "def get_date_block_num(dt: Union[date, datetime]):\n",
    "    m = dt.month\n",
    "    y = dt.year\n",
    "    return (y - 2013) * 12 + (m - 1)\n",
    "\n",
    "\n",
    "def run(fn_args):\n",
    "    fn, args, kwargs = fn_args\n",
    "    return fn(*args, **kwargs)\n",
    "\n",
    "\n",
    "def compute_concurrently(args, process_count=4):\n",
    "\n",
    "    with Pool(processes=process_count) as pool:\n",
    "        output = pool.map(run, args)\n",
    "\n",
    "    df = pd.concat(output, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_items_in_market(sales_df, block_num):\n",
    "    \"\"\"\n",
    "    Returns list of item_ids which are traded from the first day of given month\n",
    "    \"\"\"\n",
    "    last_trading_month = sales_df.groupby(['item_id'])['date_block_num'].max()\n",
    "    first_trading_month = sales_df.groupby(['item_id'])['date_block_num'].min()\n",
    "    it1 = set(last_trading_month[last_trading_month >= block_num].index.tolist())\n",
    "    it2 = set(first_trading_month[first_trading_month <= block_num].index.tolist())\n",
    "    output = list(it1.intersection(it2))\n",
    "    output.sort()\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_shops_in_market(sales_df, block_num):\n",
    "    \"\"\"\n",
    "    Returns list of shop_ids which are open in that month.\n",
    "    \"\"\"\n",
    "    last_trading_month = sales_df.groupby(['shop_id'])['date_block_num'].max()\n",
    "    first_trading_month = sales_df.groupby(['shop_id'])['date_block_num'].min()\n",
    "    it1 = set(last_trading_month[last_trading_month >= block_num].index.tolist())\n",
    "    it2 = set(first_trading_month[first_trading_month <= block_num].index.tolist())\n",
    "    output = list(it1.intersection(it2))\n",
    "    output.sort()\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling features: aggregate on past num_months data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sortedcontainers import SortedList\n",
    "\n",
    "\n",
    "def nmonths_features(sales_df, col_name, num_months, quantiles):\n",
    "    \"\"\"\n",
    "    It does not use that month's data to compute features. it uses previous months data.\n",
    "    Args:\n",
    "        sales_df: it  is sorted by shop_id, item_id and date_block_num.\n",
    "            Also, shop_item_group value changes when either the shop or the item changes.\n",
    "    \"\"\"\n",
    "    assert num_months >= 1\n",
    "    columns = ['sum', 'min', 'max'] + ['{}_q'.format(q) for q in quantiles]\n",
    "    columns = ['{}_{}M_{}'.format(col_name, num_months, c) for c in columns]\n",
    "\n",
    "    # For fast processing\n",
    "    values = sales_df[col_name].values\n",
    "    indices = sales_df.index.tolist()\n",
    "\n",
    "    output_data = np.zeros((len(indices), len(columns)))\n",
    "    s_list = SortedList()\n",
    "\n",
    "    rolling_sum = 0\n",
    "    tail_index_position = 0\n",
    "    head_index_position = -1\n",
    "    tail_index = indices[tail_index_position]\n",
    "    tail_dbn = sales_df.iloc[0]['date_block_num']\n",
    "\n",
    "    cur_group = sales_df.iloc[0]['shop_item_group']\n",
    "    for head_index in indices:\n",
    "        head_group = sales_df.at[head_index, 'shop_item_group']\n",
    "        head_index_position += 1\n",
    "        head_dbn = sales_df.at[head_index, 'date_block_num']\n",
    "\n",
    "        if head_group != cur_group:\n",
    "            s_list = SortedList()\n",
    "            cur_group = head_group\n",
    "            tail_index = head_index\n",
    "            tail_index_position = head_index_position\n",
    "            tail_dbn = sales_df.at[head_index, 'date_block_num']\n",
    "            rolling_sum = 0\n",
    "        else:\n",
    "            if head_index_position >= 1:\n",
    "                item = values[head_index_position - 1]\n",
    "                s_list.add(item)\n",
    "                rolling_sum += item\n",
    "\n",
    "        while head_dbn - tail_dbn > num_months:\n",
    "\n",
    "            value_to_be_removed = values[tail_index_position]\n",
    "            s_list.remove(value_to_be_removed)\n",
    "            rolling_sum -= value_to_be_removed\n",
    "\n",
    "            tail_index_position += 1\n",
    "            tail_index = indices[tail_index_position]\n",
    "            tail_dbn = sales_df.at[tail_index, 'date_block_num']\n",
    "\n",
    "        if len(s_list) == 0:\n",
    "            output_data[head_index_position, :] = [0] * output_data.shape[1]\n",
    "        else:\n",
    "            # compute values on data starting at tail_index_position and ending at head_index_position, both inclusive.\n",
    "            quants = [s_list[max(0, int(len(s_list) * q) - 1)] for q in quantiles]\n",
    "            output_data[head_index_position, :] = [rolling_sum, s_list[0], s_list[-1]] + quants\n",
    "\n",
    "    return pd.DataFrame(output_data, columns=columns, index=sales_df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged Features\n",
    "Given any feature, it will return what was the feature's value n months back, where n is a parameter specified in `lags`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def add_lagged_features(df, feature_name, lags=[3, 6, 12]):\n",
    "    df = df.reset_index()\n",
    "    new_features = []\n",
    "    for month_lag in lags:\n",
    "        lag_f = df[['item_id', 'shop_id', feature_name, 'date_block_num']].copy()\n",
    "        lag_f['date_block_num'] = lag_f['date_block_num'] + month_lag - 1\n",
    "        new_fname = '{}_{}M'.format(feature_name, month_lag)\n",
    "        new_features.append(new_fname)\n",
    "        lag_f.rename({feature_name: new_fname}, inplace=True, axis=1)\n",
    "        df = pd.merge(df, lag_f, how='left', on=['item_id', 'shop_id', 'date_block_num'])\n",
    "        print('{} Month lagged feature computed.'.format(month_lag))\n",
    "\n",
    "    df[new_features] = df[new_features].fillna(-10)\n",
    "    return df.set_index('index')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Price based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "INVALID_VALUE = -10\n",
    "\n",
    "\n",
    "def get_price_features(sales_df, X_df):\n",
    "    return _get_price_features(sales_df, X_df, 'item_price')\n",
    "\n",
    "\n",
    "def get_dollar_value_features(sales_df, X_df):\n",
    "    sales_df['dollar_value'] = (sales_df['item_price'] * sales_df['item_cnt_day']).astype(np.float32)\n",
    "    output_df = _get_price_features(sales_df, X_df, 'dollar_value')\n",
    "    sales_df.drop('dollar_value', axis=1, inplace=True)\n",
    "    return output_df\n",
    "\n",
    "\n",
    "def _get_price_features(sales_df, X_df, price_col):\n",
    "    \"\"\"\n",
    "    sales_df is monthly\n",
    "    \"\"\"\n",
    "    # use original sales data.\n",
    "    msg = 'X_df has >1 recent months data. To speed up the process, we are just handling 1 month into the future case'\n",
    "    assert X_df.date_block_num.max() <= sales_df.date_block_num.max() + 1, msg\n",
    "\n",
    "    sales_df = sales_df[sales_df.item_cnt_day > 0].copy()\n",
    "    sales_df.loc[sales_df[price_col] < 0, price_col] = 0\n",
    "\n",
    "    grp = sales_df.groupby(['item_id', 'date_block_num'])[price_col]\n",
    "    # std for 1 entry should be 0. std for 0 entry should be -10\n",
    "    std = grp.std().fillna(0).unstack()\n",
    "    std[sales_df.date_block_num.max() + 1] = 0\n",
    "    std = std.sort_index(axis=1).fillna(method='ffill', axis=1).shift(1, axis=1).fillna(INVALID_VALUE)\n",
    "    std_col = 'std_{}'.format(price_col)\n",
    "    std = std.stack().to_frame(std_col).reset_index()\n",
    "\n",
    "    avg_price = grp.mean().unstack()\n",
    "    avg_price[sales_df.date_block_num.max() + 1] = 0\n",
    "    avg_price = avg_price.sort_index(axis=1).fillna(method='ffill', axis=1).shift(1, axis=1).fillna(INVALID_VALUE)\n",
    "    avg_col = 'avg_{}'.format(price_col)\n",
    "    avg_price = avg_price.stack().to_frame(avg_col).reset_index()\n",
    "\n",
    "    last_price_df = sales_df[['item_id', 'shop_id', 'date_block_num', price_col]].copy()\n",
    "    last_price_df['date_block_num'] += 1\n",
    "    last_pr_col = 'last_{}'.format(price_col)\n",
    "    last_price_df.rename({price_col: last_pr_col}, inplace=True, axis=1)\n",
    "\n",
    "    # index\n",
    "    X_df = X_df.reset_index()\n",
    "\n",
    "    # item_id price\n",
    "    X_df = pd.merge(X_df, avg_price, on=['item_id', 'date_block_num'], how='left')\n",
    "    X_df[avg_col] = X_df[avg_col].fillna(INVALID_VALUE)\n",
    "\n",
    "    # shop_id item_id coupled price\n",
    "    X_df = pd.merge(X_df, last_price_df, on=['item_id', 'shop_id', 'date_block_num'], how='left')\n",
    "\n",
    "    X_df[last_pr_col] = X_df[last_pr_col].fillna(X_df[avg_col])\n",
    "\n",
    "    # stdev\n",
    "    X_df = pd.merge(X_df, std, on=['item_id', 'date_block_num'], how='left')\n",
    "    X_df[std_col] = X_df[std_col].fillna(INVALID_VALUE)\n",
    "\n",
    "    X_df.set_index('index', inplace=True)\n",
    "    X_df[[std_col, last_pr_col, avg_col]] = X_df[[std_col, last_pr_col, avg_col]].astype(np.float32)\n",
    "\n",
    "    price_category = np.log1p(X_df[last_pr_col] - INVALID_VALUE)\n",
    "\n",
    "    # CATEGORY FEATURES.\n",
    "    categ_col = 'category_{}'.format(price_col)\n",
    "    subcateg_col = 'sub_category_{}'.format(price_col)\n",
    "\n",
    "    avg_categ_col = 'avg_category_{}'.format(price_col)\n",
    "    avg_subcateg_col = 'avg_sub_category_{}'.format(price_col)\n",
    "\n",
    "    X_df[categ_col] = price_category.astype(int)\n",
    "    X_df[subcateg_col] = (price_category - X_df[categ_col]).astype(np.float32)\n",
    "\n",
    "    avg_price_category = np.log1p(X_df[avg_col] - INVALID_VALUE)\n",
    "    X_df[avg_categ_col] = avg_price_category.astype(int)\n",
    "    X_df[avg_subcateg_col] = (avg_price_category - X_df[categ_col]).astype(np.float32)\n",
    "\n",
    "    return X_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from numeric_utils import compute_concurrently\n",
    "# from numeric_rolling_features import nmonths_features\n",
    "# from price_features import get_price_features, get_dollar_value_features\n",
    "\n",
    "\n",
    "def get_y(sales_df):\n",
    "    \"\"\"\n",
    "    Note that we are not using this month's data in features. we are using previous month's data.\n",
    "    \"\"\"\n",
    "    return sales_df['item_cnt_day']\n",
    "\n",
    "\n",
    "def date_preprocessing(sales_df):\n",
    "    sales_df['month'] = sales_df['date_block_num'] % 12 + 1\n",
    "    sales_df['year'] = (sales_df['date_block_num'] // 12 + 2013).astype(np.int16)\n",
    "\n",
    "\n",
    "def basic_preprocessing(sales_df):\n",
    "    date_preprocessing(sales_df)\n",
    "    sales_df.sort_values(['shop_id', 'item_id', 'date_block_num'], inplace=True)\n",
    "    shop_id_changed = sales_df.shop_id.diff() != 0\n",
    "    item_id_changed = sales_df.item_id.diff() != 0\n",
    "    ids_changed = shop_id_changed | item_id_changed\n",
    "    sales_df['shop_item_group'] = ids_changed.cumsum()\n",
    "\n",
    "\n",
    "def get_numeric_rolling_feature_df(sales_df, process_count=4):\n",
    "    basic_preprocessing(sales_df)\n",
    "\n",
    "    sales_df['log_p'] = np.log(sales_df['item_price'])\n",
    "    quantiles = [0.25, 0.5, 0.75, 0.9]\n",
    "\n",
    "    sales_1M_features_args = [(nmonths_features, (sales_df, 'item_cnt_day', 1, quantiles), {})]\n",
    "    sales_2M_features_args = [(nmonths_features, (sales_df, 'item_cnt_day', 2, quantiles), {})]\n",
    "    sales_4M_features_args = [(nmonths_features, (sales_df, 'item_cnt_day', 4, quantiles), {})]\n",
    "\n",
    "    args = sales_1M_features_args\n",
    "    args += sales_2M_features_args\n",
    "    args += sales_4M_features_args\n",
    "\n",
    "    df = compute_concurrently(args, process_count=process_count).astype('float32')\n",
    "\n",
    "    df['month'] = sales_df['month'].astype('uint8')\n",
    "    df['year'] = sales_df['year'] - 2013\n",
    "    df['shop_id'] = sales_df['shop_id'].astype('uint8')\n",
    "    df['item_id'] = sales_df['item_id'].astype('uint16')\n",
    "    df['item_category_id'] = sales_df['item_category_id'].astype('uint8')\n",
    "\n",
    "    # std on first date is NaN\n",
    "    print('Number of nan elements', df.isna().sum().sum())\n",
    "    return df\n",
    "\n",
    "\n",
    "class NumericFeatures:\n",
    "    def __init__(self, sales_df, items_df):\n",
    "        self._sales_df = sales_df\n",
    "        self._items_df = items_df\n",
    "\n",
    "        basic_preprocessing(sales_df)\n",
    "\n",
    "    def get(self, sales_df):\n",
    "        # assert sales_df[sales_df.item_id.isin([83, 173])].empty\n",
    "\n",
    "        df = get_numeric_rolling_feature_df(sales_df)\n",
    "        print('Numeric rolling feature added.')\n",
    "\n",
    "        # price features\n",
    "        df['date_block_num'] = sales_df['date_block_num'].astype('uint8')\n",
    "        df = get_price_features(sales_df, df)\n",
    "        print('Price features added')\n",
    "\n",
    "        df = get_dollar_value_features(sales_df, df)\n",
    "        print('Dollar value features added')\n",
    "\n",
    "        df.drop('date_block_num', axis=1, inplace=True)\n",
    "\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top level code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model_validator import ModelValidator\n",
    "# from rolling_mean_encoding import rolling_mean_encoding\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "# from constants import (DATA_FPATH, TEST_LIKE_SALES_FPATH, SALES_FPATH, ITEMS_FPATH, SHOPS_FPATH, TEST_SALES_FPATH,\n",
    "#                        ITEM_CATEGORIES_FPATH)\n",
    "\n",
    "# from numeric_utils import get_date_block_num\n",
    "# from numeric_features import NumericFeatures, get_y, date_preprocessing\n",
    "# from id_features import IdFeatures\n",
    "# from lagged_features import add_lagged_features\n",
    "# from city_data_features import add_city_data_features\n",
    "\n",
    "\n",
    "class ModelData:\n",
    "    EPSILON = 1e-4\n",
    "\n",
    "    def __init__(self, sales_df, items_df, shops_df):\n",
    "        self._sales_df = sales_df\n",
    "        self._items_df = items_df\n",
    "        self._shops_df = shops_df\n",
    "\n",
    "        # adding items_category_id to dataframe.\n",
    "        item_to_cat_dict = self._items_df.set_index('item_id')['item_category_id'].to_dict()\n",
    "        self._sales_df['item_category_id'] = self._sales_df.item_id.map(item_to_cat_dict)\n",
    "\n",
    "        self._numeric_features = NumericFeatures(self._sales_df, self._items_df)\n",
    "\n",
    "        # orig_item_id is needed in id_features.\n",
    "        self._sales_df['orig_item_id'] = self._sales_df['item_id']\n",
    "        self._id_features = IdFeatures(self._sales_df, self._items_df)\n",
    "\n",
    "    def get_train_X_y(self):\n",
    "        print('Fetching X')\n",
    "        X_df = self.get_X(self._sales_df)\n",
    "        print('X fetched. Fetching y')\n",
    "        y_df = get_y(self._sales_df).to_frame('item_cnt_month')\n",
    "        print('Y fetched')\n",
    "\n",
    "        # Retain common rows\n",
    "        X_df = X_df.join(y_df[[]], how='inner')\n",
    "        y_df = y_df.join(X_df[[]], how='inner')['item_cnt_month']\n",
    "        # Order\n",
    "        y_df = y_df.loc[X_df.index]\n",
    "        return (X_df, y_df)\n",
    "\n",
    "    def get_X(self, sales_df):\n",
    "        X_df = self._numeric_features.get(sales_df)\n",
    "        assert X_df.index.equals(sales_df.index)\n",
    "        print('Numeric features fetched.')\n",
    "\n",
    "        X_df['orig_item_id'] = sales_df['orig_item_id']\n",
    "        X_df['date_block_num'] = sales_df['date_block_num']\n",
    "\n",
    "        # # add first month related features for item_id\n",
    "        X_df = self._id_features.get_fm_features(X_df, item_id_and_shop_id=False)\n",
    "        # add first month related features for item_id and shop_id jointly\n",
    "        X_df = self._id_features.get_fm_features(X_df, item_id_and_shop_id=True)\n",
    "\n",
    "        print('Id features added')\n",
    "\n",
    "        # lagged features added\n",
    "        X_df = add_lagged_features(X_df, 'item_cnt_day_1M_sum')\n",
    "        print('Lagged features added')\n",
    "\n",
    "        # City related features like area, coordinate, city_id\n",
    "        X_df = add_city_data_features(X_df, self._shops_df)\n",
    "        print('City data features added')\n",
    "\n",
    "        return X_df\n",
    "\n",
    "    def get_test_X(self, sales_test_df, test_datetime: datetime):\n",
    "\n",
    "        item_id_original = sales_test_df['item_id'].copy()\n",
    "        # adding items_category_id to dataframe.\n",
    "        item_to_cat_dict = self._items_df.set_index('item_id')['item_category_id'].to_dict()\n",
    "        sales_test_df['item_category_id'] = sales_test_df.item_id.map(item_to_cat_dict)\n",
    "\n",
    "        sales_test_df['date'] = test_datetime.strftime('%d.%m.%Y')\n",
    "        sales_test_df['date_block_num'] = get_date_block_num(test_datetime)\n",
    "        sales_test_df['orig_item_id'] = sales_test_df['item_id']\n",
    "\n",
    "        sales_test_df['item_cnt_day'] = 0\n",
    "        sales_test_df['item_price'] = 0\n",
    "\n",
    "        date_preprocessing(sales_test_df)\n",
    "        assert sales_test_df.loc[item_id_original.index]['orig_item_id'].equals(item_id_original)\n",
    "\n",
    "        test_dbn = get_date_block_num(test_datetime)\n",
    "        recent_sales_df = self._sales_df[self._sales_df.date_block_num < test_dbn]\n",
    "\n",
    "        recent_sales_df = recent_sales_df.drop('shop_item_group', axis=1)\n",
    "\n",
    "        subtract_index_offset = max(recent_sales_df.index) - (min(sales_test_df.index) - 1)\n",
    "        recent_sales_df.index -= subtract_index_offset\n",
    "\n",
    "        df = pd.concat([recent_sales_df, sales_test_df], axis=0, sort=False)\n",
    "\n",
    "        assert df.loc[item_id_original.index]['orig_item_id'].equals(item_id_original)\n",
    "\n",
    "        print('Preprocessing X about to be done now.')\n",
    "        X_df = self.get_X(df)\n",
    "\n",
    "        X_df = X_df.loc[sales_test_df.index]\n",
    "\n",
    "        assert X_df.loc[item_id_original.index]['orig_item_id'].equals(item_id_original)\n",
    "        return X_df\n",
    "\n",
    "\n",
    "def mean_encoding_preprocessing(sales):\n",
    "    test_sales_df = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv', index_col=0)\n",
    "    test_sales_df['date_block_num'] = 34\n",
    "    test_sales_df['item_cnt_day'] = 0.33\n",
    "    index_offset = sales.index.max() + 1\n",
    "    print('Offsetting test sales index by ', index_offset)\n",
    "    test_sales_df.index += index_offset\n",
    "    combined_sales_df = pd.concat(\n",
    "        [sales[['date_block_num', 'item_id', 'shop_id', 'item_cnt_day']], test_sales_df], sort=True)\n",
    "    combined_sales_df = pd.merge(\n",
    "        combined_sales_df.reset_index(), items[['item_id', 'item_category_id']], on='item_id',\n",
    "        how='left').set_index('index')\n",
    "\n",
    "    combined_sales_df = rolling_mean_encoding(combined_sales_df)\n",
    "    train_mean_encoding = combined_sales_df.loc[sales.index].copy()\n",
    "    test_mean_encoding = combined_sales_df.loc[test_sales_df.index].copy()\n",
    "    test_mean_encoding.index = test_mean_encoding.index - (sales.index.max() + 1)\n",
    "    assert test_mean_encoding.shape[0] + train_mean_encoding.shape[0] == combined_sales_df.shape[0]\n",
    "    assert (test_mean_encoding.date_block_num == 34).all()\n",
    "\n",
    "    return (train_mean_encoding, test_mean_encoding)\n",
    "\n",
    "\n",
    "def get_val_dfs(skip_last_n_months: int):\n",
    "    \"\"\"\n",
    "    skip_last_n_months decides how many months prior to Nov 2015 to take as the validation data.\n",
    "    \"\"\"\n",
    "    year = (1 + 33 - skip_last_n_months) // 12 + 2013\n",
    "    month = (1 + 33 - skip_last_n_months) % 12\n",
    "\n",
    "    mv = ModelValidator(sales, X_df, y_df, skip_last_n_months=skip_last_n_months)\n",
    "    val_X_ids, val_y_df = mv.get_val_data()\n",
    "    print('Before get_test_X', val_X_ids.shape, val_y_df.shape)\n",
    "    val_X_df = md.get_test_X(val_X_ids, datetime(year, month, 1))\n",
    "    assert val_X_df.index.equals(val_X_ids.index)\n",
    "    print('After get_test_X', val_X_df.shape)\n",
    "    return (val_X_df, val_y_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching X\n",
      "Number of nan elements 0\n",
      "Numeric rolling feature added.\n",
      "Price features added\n",
      "Dollar value features added\n",
      "Numeric features fetched.\n",
      "Id features added\n",
      "3 Month lagged feature computed.\n",
      "6 Month lagged feature computed.\n",
      "12 Month lagged feature computed.\n",
      "Lagged features added\n",
      "City data features added\n",
      "X fetched. Fetching y\n",
      "Y fetched\n"
     ]
    }
   ],
   "source": [
    "# t_sales = sales[sales.date_block_num >=30].copy()\n",
    "# md = ModelData(t_sales, items, shops)\n",
    "md = ModelData(sales, items, shops)\n",
    "X_df, y_df= md.get_train_X_y()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not X_df.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing X about to be done now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:32: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nan elements 0\n",
      "Numeric rolling feature added.\n",
      "Price features added\n",
      "Dollar value features added\n",
      "Numeric features fetched.\n",
      "Id features added\n",
      "3 Month lagged feature computed.\n",
      "6 Month lagged feature computed.\n",
      "12 Month lagged feature computed.\n",
      "Lagged features added\n",
      "City data features added\n"
     ]
    }
   ],
   "source": [
    "# t_test = test[test.shop_id.isin([1,2,3,4,5,6,7,8,9])].copy()\n",
    "# test_df = md.get_test_X(t_test, datetime(2015,11,1))\n",
    "\n",
    "test_df = md.get_test_X(test, datetime(2015,11,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(214200, 54)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_df.loc[test.index].orig_item_id.equals(test.item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8049107, 54) (8049107,)\n"
     ]
    }
   ],
   "source": [
    "print(X_df.shape, y_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(214200, 54)\n",
      "(214200, 11)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "\n",
    "def _rolling_mean_encoding_by_id(combined_sales_df, col_id):\n",
    "    if col_id == 'item_shop_id':\n",
    "        assert combined_sales_df.groupby(\n",
    "            [col_id, 'date_block_num'])['item_cnt_day'].count().shape[0] == combined_sales_df.shape[0]\n",
    "        df = combined_sales_df[['item_cnt_day', 'item_shop_id', 'date_block_num']].rename(\n",
    "            {\n",
    "                'item_cnt_day': 'item_cnt_month'\n",
    "            }, axis=1)\n",
    "    else:\n",
    "        df = combined_sales_df.groupby(\n",
    "            [col_id, 'date_block_num'])['item_cnt_day'].mean().to_frame('item_cnt_month').reset_index()\n",
    "\n",
    "    df.sort_values('date_block_num', inplace=True)\n",
    "\n",
    "    sum_df = df.groupby(col_id)['item_cnt_month'].cumsum() - df['item_cnt_month']\n",
    "    count_df = df.groupby(col_id)['item_cnt_month'].cumcount()\n",
    "    id_encoding = sum_df / count_df\n",
    "    id_encoding[count_df == 0] = -10\n",
    "    id_encoding = id_encoding.to_frame(col_id + '_enc').astype(np.float32)\n",
    "\n",
    "    assert id_encoding.index.equals(df.index)\n",
    "    id_encoding['date_block_num'] = df['date_block_num']\n",
    "    id_encoding[col_id] = df[col_id]\n",
    "    return id_encoding\n",
    "\n",
    "\n",
    "def _rolling_quantile_encoding_by_id(combined_sales_df, col_id, quantile):\n",
    "    assert col_id != 'item_shop_id'\n",
    "    df = combined_sales_df.groupby(\n",
    "        [col_id, 'date_block_num'])['item_cnt_day'].quantile(quantile).to_frame('item_cnt_month').reset_index()\n",
    "\n",
    "    df.sort_values('date_block_num', inplace=True)\n",
    "\n",
    "    # Alternate formulation.\n",
    "    # df.groupby(3)[1].rolling(10, min_periods=1).quantile(0).reset_index(level=0)\n",
    "\n",
    "    sum_df = df.groupby(col_id)['item_cnt_month'].cumsum() - df['item_cnt_month']\n",
    "    count_df = df.groupby(col_id)['item_cnt_month'].cumcount()\n",
    "    id_encoding = sum_df / count_df\n",
    "    id_encoding[count_df == 0] = -10\n",
    "    id_encoding = id_encoding.to_frame(col_id + '_qt_{}_enc'.format(quantile)).astype(np.float32)\n",
    "\n",
    "    assert id_encoding.index.equals(df.index)\n",
    "    id_encoding['date_block_num'] = df['date_block_num']\n",
    "    id_encoding[col_id] = df[col_id]\n",
    "    return id_encoding\n",
    "\n",
    "\n",
    "def rolling_mean_encoding(combined_sales_df):\n",
    "\n",
    "    int64 = combined_sales_df[['item_id', 'shop_id', 'item_category_id']].dtypes == np.int64\n",
    "    int32 = combined_sales_df[['item_id', 'shop_id', 'item_category_id']].dtypes == np.int32\n",
    "    assert (int32 | int64).all()\n",
    "\n",
    "    orig_index = combined_sales_df.index\n",
    "    combined_sales_df.reset_index(inplace=True)\n",
    "\n",
    "    # item shop_encoding\n",
    "    combined_sales_df['item_shop_id'] = combined_sales_df['item_id'] * 100 + combined_sales_df['shop_id']\n",
    "    # shop category id\n",
    "    combined_sales_df['shop_category_id'] = combined_sales_df['shop_id'] * 100 + combined_sales_df['item_category_id']\n",
    "\n",
    "    for col_id in ['item_id', 'item_category_id', 'shop_id', 'item_shop_id', 'shop_category_id']:\n",
    "        # item_id\n",
    "        encoding = _rolling_mean_encoding_by_id(combined_sales_df, col_id)\n",
    "        combined_sales_df = pd.merge(combined_sales_df, encoding, on=[col_id, 'date_block_num'], how='left')\n",
    "        print('{} encoding completed'.format(col_id))\n",
    "\n",
    "    pbar = tqdm_notebook([0.1, 0.9, 0.95])\n",
    "    for qt in pbar:\n",
    "        for col_id in tqdm_notebook(['item_id', 'item_category_id', 'shop_id', 'shop_category_id']):\n",
    "            pbar.set_description('Running {} Quantile mean encoding for \"{}\"'.format(qt, col_id))\n",
    "            # item_id\n",
    "            encoding = _rolling_quantile_encoding_by_id(combined_sales_df, col_id, qt)\n",
    "            combined_sales_df = pd.merge(combined_sales_df, encoding, on=[col_id, 'date_block_num'], how='left')\n",
    "            print('{} encoding completed'.format(col_id))\n",
    "\n",
    "    combined_sales_df.drop(['item_shop_id', 'shop_category_id'], axis=1, inplace=True)\n",
    "    combined_sales_df.set_index('index', inplace=True)\n",
    "\n",
    "    return combined_sales_df.loc[orig_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offsetting test sales index by  9375832\n"
     ]
    }
   ],
   "source": [
    "test_sales_df =pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv', index_col=0)\n",
    "test_sales_df['date_block_num'] = 34\n",
    "test_sales_df['item_cnt_day'] = 0.33\n",
    "index_offset = sales.index.max() + 1\n",
    "print('Offsetting test sales index by ', index_offset)\n",
    "test_sales_df.index += index_offset\n",
    "combined_sales_df = pd.concat([sales[['date_block_num','item_id','shop_id','item_cnt_day']], test_sales_df],sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_sales_df = pd.merge(combined_sales_df.reset_index(), items[['item_id','item_category_id']],\n",
    "                             on='item_id',how='left').set_index('index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id encoding completed\n",
      "item_category_id encoding completed\n",
      "shop_id encoding completed\n",
      "item_shop_id encoding completed\n",
      "shop_category_id encoding completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2afd6f62e640498293de7af8010140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4643c23c4a144a6bb4f25cc32411f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id encoding completed\n",
      "item_category_id encoding completed\n",
      "shop_id encoding completed\n",
      "shop_category_id encoding completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e84b05289fd45ee80542f4503f81dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id encoding completed\n",
      "item_category_id encoding completed\n",
      "shop_id encoding completed\n",
      "shop_category_id encoding completed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f633b6dec3494509a8f1bcfaf3f36e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id encoding completed\n",
      "item_category_id encoding completed\n",
      "shop_id encoding completed\n",
      "shop_category_id encoding completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "combined_sales_df = rolling_mean_encoding(combined_sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting offseting of test sales by  9375832\n"
     ]
    }
   ],
   "source": [
    "train_mean_encoding = combined_sales_df.loc[sales.index].copy()\n",
    "print('Adjusting offseting of test sales by ', sales.index.max() +1)\n",
    "test_mean_encoding = combined_sales_df.loc[test_sales_df.index].copy()\n",
    "test_mean_encoding.index = test_mean_encoding.index - (sales.index.max() +1)\n",
    "assert test_mean_encoding.shape[0] + train_mean_encoding.shape[0] == combined_sales_df.shape[0]\n",
    "assert (test_mean_encoding.date_block_num == 34).all()\n",
    "\n",
    "test_mean_encoding.drop(['date_block_num','item_cnt_day','item_id','shop_id','item_category_id'],axis=1,inplace=True)\n",
    "train_mean_encoding.drop(['date_block_num','item_cnt_day','item_id','shop_id','item_category_id'],axis=1,inplace=True)\n",
    "del combined_sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not test_mean_encoding.isna().any().any()\n",
    "assert not train_mean_encoding.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not X_df.isna().any().any()\n",
    "X_df = pd.concat([X_df, train_mean_encoding], axis=1)\n",
    "assert not X_df.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not test_df.isna().any().any()\n",
    "test_df = pd.concat([test_df, test_mean_encoding], axis=1)\n",
    "assert not test_df.isna().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df.to_hdf(DATA_FNAME,'X')\n",
    "y_df.to_hdf(DATA_FNAME, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_hdf(DATA_FNAME,'test_X')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
