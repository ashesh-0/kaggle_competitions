{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "This notebook simply concatenates the numeric features with text generated features.\n",
    "Since the data size was getting large, a separate notebook was needed to combine the text generated features with all other features. Doing it all in same notebook was creating memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['addingzerototrain', 'fork-of-treebasedapproachdata', 'nn-features', 'competitive-data-science-predict-future-sales', 'textfeatures']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import gc\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMBINED_DATA_FPATH='DATA_WITH_TXT.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FPATH = '../input/nn-features/NNDATA.hdf'\n",
    "TEST_LIKE_SALES_FPATH = '../input/addingzerototrain/train_with_zero.hdf'\n",
    "SALES_FPATH ='../input/competitive-data-science-predict-future-sales/sales_train.csv'\n",
    "ITEMS_FPATH = '../input/competitive-data-science-predict-future-sales/items.csv'\n",
    "SHOPS_FPATH = '../input/competitive-data-science-predict-future-sales/shops.csv'\n",
    "TEST_SALES_FPATH = '../input/competitive-data-science-predict-future-sales/test.csv'\n",
    "SAMPLE_SUBMISSION_FPATH = '../input/competitive-data-science-predict-future-sales/sample_submission.csv'\n",
    "TRAINED_MODEL_FPATH = 'trained_model.bin'\n",
    "\n",
    "TEXT_FEATURE_FPATH = '../input/textfeatures/text_features.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data.\n",
    "X_df = pd.read_hdf(DATA_FPATH, 'X')\n",
    "y_df = pd.read_hdf(DATA_FPATH, 'y')\n",
    "\n",
    "sales_df = pd.read_hdf(TEST_LIKE_SALES_FPATH, 'df')\n",
    "\n",
    "item_text = pd.read_hdf(TEXT_FEATURE_FPATH, 'item_500')\n",
    "shop_text = pd.read_hdf(TEXT_FEATURE_FPATH, 'shop_50')\n",
    "category_text = pd.read_hdf(TEXT_FEATURE_FPATH, 'category_60')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "float64_cols = X_df.dtypes[X_df.dtypes == np.float64].index.tolist()\n",
    "if float64_cols:\n",
    "    X_df[float64_cols] = X_df[float64_cols].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_df = X_df.isna().any().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_df.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['item_cnt_day_1M_sum', 'item_cnt_day_1M_min', 'item_cnt_day_1M_max',\n",
       "       'item_cnt_day_1M_0.25_q', 'item_cnt_day_1M_0.5_q',\n",
       "       'item_cnt_day_1M_0.75_q', 'item_cnt_day_1M_0.9_q',\n",
       "       'item_cnt_day_2M_sum', 'item_cnt_day_2M_min', 'item_cnt_day_2M_max',\n",
       "       'item_cnt_day_2M_0.25_q', 'item_cnt_day_2M_0.5_q',\n",
       "       'item_cnt_day_2M_0.75_q', 'item_cnt_day_2M_0.9_q',\n",
       "       'item_cnt_day_4M_sum', 'item_cnt_day_4M_min', 'item_cnt_day_4M_max',\n",
       "       'item_cnt_day_4M_0.25_q', 'item_cnt_day_4M_0.5_q',\n",
       "       'item_cnt_day_4M_0.75_q', 'item_cnt_day_4M_0.9_q', 'month', 'year',\n",
       "       'shop_id', 'item_id', 'item_category_id', 'avg_item_price',\n",
       "       'last_item_price', 'std_item_price', 'category_item_price',\n",
       "       'sub_category_item_price', 'avg_category_item_price',\n",
       "       'avg_sub_category_item_price', 'avg_dollar_value', 'last_dollar_value',\n",
       "       'std_dollar_value', 'category_dollar_value',\n",
       "       'sub_category_dollar_value', 'avg_category_dollar_value',\n",
       "       'avg_sub_category_dollar_value', 'orig_item_id', 'date_block_num',\n",
       "       'orig_item_id_oldness', 'orig_item_id_is_fm',\n",
       "       'orig_item_id_shop_id_oldness', 'orig_item_id_shop_id_is_fm',\n",
       "       'item_cnt_day_1M_sum_3M', 'item_cnt_day_1M_sum_6M',\n",
       "       'item_cnt_day_1M_sum_12M', 'city_id', 'city_lat', 'city_lon',\n",
       "       'city_importance', 'city_area', 'item_id_enc', 'item_category_id_enc',\n",
       "       'shop_id_enc', 'item_shop_id_enc', 'shop_category_id_enc',\n",
       "       'item_id_qt_0.1_enc', 'item_category_id_qt_0.1_enc',\n",
       "       'shop_id_qt_0.1_enc', 'shop_category_id_qt_0.1_enc',\n",
       "       'item_id_qt_0.9_enc', 'item_category_id_qt_0.9_enc',\n",
       "       'shop_id_qt_0.9_enc', 'shop_category_id_qt_0.9_enc',\n",
       "       'item_id_qt_0.95_enc', 'item_category_id_qt_0.95_enc',\n",
       "       'shop_id_qt_0.95_enc', 'shop_category_id_qt_0.95_enc',\n",
       "       '1Neighbor_std_dollar_value', '2Neighbor_std_dollar_value',\n",
       "       '6Neighbor_std_dollar_value', '1Neighbor_item_cnt_day_2M_sum',\n",
       "       '2Neighbor_item_cnt_day_2M_sum', '6Neighbor_item_cnt_day_2M_sum',\n",
       "       '1Neighbor_item_cnt_day_1M_sum', '2Neighbor_item_cnt_day_1M_sum',\n",
       "       '6Neighbor_item_cnt_day_1M_sum'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target value cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df[y_df > 20 ]  = 20\n",
    "y_df[y_df < 0] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling data types for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cols =['item_name_text_{}'.format(i) for i in range(50)]\n",
    "item_text = item_text[['item_id'] + t_cols]\n",
    "item_text[t_cols] = item_text[t_cols].astype(np.float32)\n",
    "\n",
    "t_cols = ['shop_name_text_{}'.format(i) for i in range(8)]\n",
    "shop_text = shop_text[['shop_id'] + t_cols]\n",
    "shop_text[t_cols] = shop_text[t_cols].astype(np.float32)\n",
    "\n",
    "t_cols =['item_category_name_text_{}'.format(i) for i in range(5)]\n",
    "category_text = category_text[['item_category_id'] + t_cols]\n",
    "category_text[t_cols] = category_text[t_cols].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_features(df):\n",
    "    df.reset_index(inplace=True)\n",
    "    df = pd.merge(df, shop_text, how='left', on='shop_id')\n",
    "    print('Shop text added')\n",
    "    gc.collect()\n",
    "    df = pd.merge(df, category_text, how='left', on='item_category_id')\n",
    "    print('Category text added')\n",
    "    gc.collect()\n",
    "    df = pd.merge(df, item_text, how='left', on='item_id')\n",
    "    print('Item text added')\n",
    "    gc.collect()\n",
    "    df.set_index('index',inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature concatenation\n",
    "We separate the data into years. For each year we create one file. This is done just to circumvent the out of memory issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shop text added\n",
      "Category text added\n",
      "Item text added\n",
      "Shop text added\n",
      "Category text added\n",
      "Item text added\n",
      "Shop text added\n",
      "Category text added\n",
      "Item text added\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df.to_hdf(COMBINED_DATA_FPATH, 'y')\n",
    "del y_df\n",
    "\n",
    "for year in X_df.year.unique():\n",
    "    X_train_df = X_df[X_df.year == year].copy()\n",
    "    X_train_df = add_text_features(X_train_df)\n",
    "    gc.collect()\n",
    "    train_columns = X_train_df.columns.tolist()\n",
    "    X_train_df.to_hdf(COMBINED_DATA_FPATH,'X_{}'.format(year))    \n",
    "    del X_train_df\n",
    "    gc.collect()\n",
    "\n",
    "del X_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature concatenation for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shop text added\n",
      "Category text added\n",
      "Item text added\n"
     ]
    }
   ],
   "source": [
    "test_X_df = pd.read_hdf(DATA_FPATH, 'test_X')\n",
    "test_X_df = add_text_features(test_X_df)\n",
    "test_X_df = test_X_df[train_columns]\n",
    "if float64_cols:\n",
    "    test_X_df[float64_cols] = test_X_df[float64_cols].astype(np.float32)\n",
    "test_X_df.to_hdf(COMBINED_DATA_FPATH, 'X_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
